# 01_retail_data

```
[train@trainvm ~]$ start-all.sh
```
```
[train@trainvm ~]$ jupyter-lab
```
```
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
```

```
spark = SparkSession.builder \
.appName("filter") \
.master("local[2]") \
.getOrCreate()
```
# 1.1 `order_items` tablosunda kaç tane **tekil** `orderItemOrderId` var
```
! wget -P /home/train/datasets/ \
https://raw.githubusercontent.com/erkansirin78/datasets/master/retail_db/order_items.csv
```

```
df_order_items = spark.read \
.format("csv") \
.option("header", True) \
.option("sep", ",") \
.option("inferSchema", True) \
.load("file:///home/train/datasets/order_items.csv")
```

```
df_order_items.show()
```

```
from pyspark.sql.functions import countDistinct
df2=df_order_items.select(countDistinct("orderItemOrderId"))
df2.show()
```

# 1.2`orders` ve `order_items` tablolarında kaç satır var
```
df_order_items.toPandas().info()
```
```
! wget -P /home/train/datasets/ \
https://raw.githubusercontent.com/erkansirin78/datasets/master/retail_db/orders.csv
```

```
df_orders = spark.read \
.format("csv") \
.option("header", True) \
.option("sep", ",") \
.option("inferSchema", True) \
.load("file:///home/train/datasets/orders.csv")
```


```
df_orders.show()
```


```
df_orders.toPandas().info()
```

# 1.3 Toplam satış tutarı bakımından en çok iptal edilen (azalan sıra) ürünleri lokal diske `parquet` formatında yazınız.

```
canceled_items = df_orders.filter(df_orders.orderStatus == "CANCELED")
```
```
joined_data = df_order_items.join(canceled_items, df_order_items.orderItemOrderId == canceled_items.orderId)
```
```
! wget -P /home/train/datasets/ \
https://raw.githubusercontent.com/erkansirin78/datasets/master/retail_db/products.csv
```

```
df_products = spark.read \
.format("csv") \
.option("header", True) \
.option("sep", ",") \
.option("inferSchema", True) \
.load("file:///home/train/datasets/products.csv")
```

```
joined_data1 = df_products.join(joined_data, df_products.productId == joined_data.orderItemProductId)
```
```
joined_data1.show()
```
```
product_sales = joined_data1.groupBy("productName").agg({"orderItemSubTotal": "sum"}).withColumnRenamed("sum(orderItemSubTotal)", "productSales")
```
```
product_sales.show(30)
```








