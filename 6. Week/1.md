### 0. Qestion

```
- New sales are available per hour in `http://trainvm.vbo.local:5000/data`

- You need to ingest this data into object storage as it is. 
  - Example:
    - For the time xx:yy you need to process hour_xx_supermarket_sales.csv file.
    - For the time 13:15 you need to process hour_13_supermarket_sales.csv file.

- For some hours there will be no data. In this case your pipeline must not fail. 

- After ingesting data from nginx server you need to save object storage:
  - Bucket: `dataops`
  - Key: `supermarket_sales/{today}/hour_{this_hour}_supermarket_sales.csv`

- For each day there must be different directory and in this directory there must be at most 24 files for each hour. 
  But if there is no data in the source, no file is acceptable.

- Example value for today: 20230312 and example value for hour: 17
```


### 1. Activate venv for airflow
 
```
[train@trainvm ~]$ source ~/venvairflow/bin/activate
```
 
```
(venvairflow) [train@trainvm ~]$ sudo systemctl start airflow
```
  
```
(venvairflow) [train@trainvm ~]$ sudo systemctl start airflow-scheduler
```

```
(venvairflow) [train@trainvm ~]$ cd dataops7/
```

```
(venvairflow) [train@trainvm ~]$ cd airflow_project/
```

```
(venvairflow) [train@trainvm airflow_project]$ sudo systemctl start docker
```

```
(venvairflow) [train@trainvm airflow_project]$ docker-compose up -d
```

```
(venvairflow) [train@trainvm airflow_project]$ docker-compose ps
```

### 2. Pre Requiests

```
# connect to nginx container
docker exec -it nginx bash


# change directory to html root
root@1ea647bbd0c4:/# cd /usr/share/nginx/html/
root@1ea647bbd0c4:/usr/share/nginx/html# mkdir data
root@1ea647bbd0c4:/usr/share/nginx/html# cd data


# Download data
root@1ea647bbd0c4:/usr/share/nginx/html/data# for i in {10..20}; do curl -o hour_${i}_supermarket_sales.csv  https://raw.githubusercontent.com/erkansirin78/datasets/master/supermarket_hourly/hour_${i}_supermarket_sales.csv; done


# check data
root@1ea647bbd0c4:/usr/share/nginx/html/data# ls -l
total 160
-rw-r--r--. 1 root root   497 Jul 19 14:05 50x.html
-rw-r--r--. 1 root root 13698 Oct 15 05:57 hour_10_supermarket_sales.csv
-rw-r--r--. 1 root root 12159 Oct 15 05:57 hour_11_supermarket_sales.csv
-rw-r--r--. 1 root root 12069 Oct 15 05:57 hour_12_supermarket_sales.csv
-rw-r--r--. 1 root root 13906 Oct 15 05:57 hour_13_supermarket_sales.csv
-rw-r--r--. 1 root root 11241 Oct 15 05:57 hour_14_supermarket_sales.csv
-rw-r--r--. 1 root root 13770 Oct 15 05:57 hour_15_supermarket_sales.csv
-rw-r--r--. 1 root root 10454 Oct 15 05:57 hour_16_supermarket_sales.csv
-rw-r--r--. 1 root root 10085 Oct 15 05:57 hour_17_supermarket_sales.csv
-rw-r--r--. 1 root root 12597 Oct 15 05:57 hour_18_supermarket_sales.csv
-rw-r--r--. 1 root root 15266 Oct 15 05:57 hour_19_supermarket_sales.csv
-rw-r--r--. 1 root root 10222 Oct 15 05:57 hour_20_supermarket_sales.csv
-rw-r--r--. 1 root root   615 Jul 19 14:05 index.html
```

- Test 
```
[train@trainvm ~]$ curl -o /tmp/hour_13_supermarket_sales.csv http://localhost:5000/data/hour_13_supermarket_sales.csv

[train@trainvm ~]$ ls -l /tmp | grep super
-rw-rw-r--. 1 train train  13906 Oct 15 09:02 hour_13_supermarket_sales.csv
```


### 3. Create "homework_6.py"
```
from datetime import datetime, timedelta
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.operators.dummy import DummyOperator

import boto3, logging, io, botocore

import pandas as pd
import boto3, logging, io

start_date = datetime(2023, 3, 14)

hour = datetime.now().hour

date = datetime.now().strftime("%Y%m%d'")

default_args = {
    'owner': 'train',
    'start_date': start_date,
    'retries': 1,
    'retry_delay': timedelta(seconds=5)
}

s3_res = boto3.resource('s3',
                        aws_access_key_id=Variable.get("s3_access_key_id"),
                        aws_secret_access_key = Variable.get("s3_secret_access_key"),
                        endpoint_url = 'http://localhost:9000')

def supermarket_to_s3():
    def get_load_data(s3_res, bucket, key):
        try:
            df = pd.read_csv("http://localhost:5888/data/hour (hour) supermarket_sales.csv")
            print(df.head())
            csv_buffer = io.String10()
            df.to_csv(csv_buffer, index=False)
            s3_res.Object(bucket, key).put(Body=csv_buffer.getvalue())
        except:
            pass

    get_load_data(s3_res, "dataops", f"{date}/hour_{hour}_supermarket_sales.csv")


with DAG("homework_6", default_args=default_args, schedule_interval="@hourly", catchup=False) as dag:

    start = DummyOperator(task_id="start_task")

    save_s3 = PythonOperator(task_id="save_s3_task", python_callable=supermarket_to_s3)

    final = DummyOperator(task_id="final_task")

    start >> save_s3 >> final
```

### 4. Create AWS key_id and access_key

- Go to http://127.0.0.1:1502/home

- Go to Admin-Variables

- Create "s3_access_key_id" and  "s3_secret_access_key"









